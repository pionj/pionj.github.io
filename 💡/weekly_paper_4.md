---
title: "weekly_paper_4"
description: "weekly_paper_4.md | (2025 June 08 Sunday)"
date: 2025-06-08T23:24:12+09:00
lastmod: 2025-06-08T23:24:12+09:00
categories: 
- 
tags: 
- 
series:
- weekly paper
draft: false
private: false
---

# Weekly Paper 4

## TOC
- [Weekly Paper 4](#)

## 🇶 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.

-   **포함 관계**: **딥러닝 ⊂ 머신러닝**. 즉, 딥러닝은 머신러닝의 한 분야입니다.
-   **머신러닝**: 데이터를 이용하여 패턴을 학습하고 예측이나 결정을 수행하는 알고리즘 및 시스템 전반을 의미합니다. (지도, 비지도, 강화학습 등 포함)
-   **딥러닝**: 머신러닝 방법 중 **인공 신경망(Artificial Neural Network, ANN)**, 특히 **여러 개의 은닉층(hidden layer)을 가진 심층 신경망(Deep Neural Network, DNN)**을 사용하는 기법들을 통칭합니다.
    *   **인공 신경망**: 인간 뇌의 뉴런 연결 구조에서 영감을 받아, 입력층, (여러 개의) 은닉층, 출력층으로 구성됩니다. 
    *   각 층의 노드(뉴런)들은 이전 층의 노드들과 가중치로 연결되어 신호를 전달하고 활성화 함수를 통해 출력을 결정합니다. (마치 작은 계산기들이 연결되어 복잡한 계산을 함께 수행하는 모습)
    *   (단일 뉴런이 시그모이드 활성화 함수를 사용하면, 이는 로지스틱 회귀 유닛과 계산 방식이 유사합니다.)
    *   **"Deep"의 의미**: 은닉층이 여러 겹으로 깊게 쌓여 있다는 뜻입니다. 이를 통해 데이터로부터 복잡하고 추상적인 특징(feature)을 **자동으로 추출하고 학습**할 수 있습니다. (예: 이미지에서 처음에는 선, 모서리 같은 단순 특징을, 다음 층에서는 눈, 코 같은 복합 특징을, 더 깊은 층에서는 얼굴 전체를 인식)

-   **발전 - 트랜스포머(Transformer)와 셀프 어텐션(Self-Attention)**:
    *   **배경**: 기존 순환 신경망(RNN)은 순차적 처리로 인한 병렬성 한계와 긴 시퀀스 정보 전달의 어려움(장기 의존성 문제)이 있었습니다.
    *   **트랜스포머**: 이러한 한계를 극복하기 위해 등장. 핵심은 **셀프 어텐션 메커니즘**입니다.
        *   **아이디어**: 입력 시퀀스 내의 각 요소(예: 단어)가 다른 모든 요소들과 얼마나 연관되어 있는지를 한 번에 계산하여 문맥 정보를 파악합니다. (마치 한 문장 내 단어들이 서로의 중요도를 동시에 평가하는 것)
        *   **과정 (간략히)**:
            1.  각 입력 요소에 대해 쿼리(Q), 키(K), 값(V) 벡터 생성.
            2.  쿼리가 다른 모든 키들과의 유사도(어텐션 스코어) 계산.
            3.  스코어를 스케일링 후 소프트맥스 함수를 통과시켜 어텐션 가중치 획득.
            4.  이 가중치를 해당 값(V) 벡터들에 곱하여 가중 합, 최종 어텐션 출력 생성.
        *   **장점**: 병렬 처리가 용이하고, 긴 의존성 관계 학습에 효과적입니다. (주로 자연어 처리(NLP) 분야에서 혁신을 가져왔으며, 컴퓨터 비전 등 다른 분야로도 확장 중)

-   **딥러닝과 다양한 학습 방식**:
    *   **지도 학습**: 이미지 분류, 객체 탐지, 기계 번역 등 대부분의 성공적인 딥러닝 모델이 지도 학습으로 훈련됩니다.
    *   **비지도 학습**:
        *   **오토인코더**: 데이터 압축 및 복원을 통해 특징 학습 (차원 축소, 이상 탐지).
        *   **GAN (생성적 적대 신경망)**: 실제 같은 데이터 생성 (이미지 생성, 스타일 변환).
        *   **자기 지도 학습 (Self-Supervised Learning)**: 레이블 없는 데이터에서 자체적으로 레이블을 만들어 학습 (BERT, GPT 등 대규모 언어 모델의 사전 학습).
    *   **강화학습**:
        *   **심층 강화학습 (DRL, Deep Reinforcement Learning)**: 딥러닝을 사용하여 복잡한 환경에서 최적의 정책(행동 규칙)을 학습 (게임 AI, 로봇 제어, 자율 주행).


## 🇶 딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

-   **하이퍼파라미터란?**: 모델이 학습을 시작하기 전에 사용자가 직접 설정해야 하는 값들로, 모델의 구조나 학습 과정에 영향을 미칩니다. (마치 요리 레시피에서 "오븐 온도 180도, 굽는 시간 20분"처럼 미리 정해주는 값)
-   **일반적인 딥러닝 하이퍼파라미터**:
    *   **학습률 (Learning Rate)**: 가중치를 업데이트하는 보폭(크기)입니다. 너무 크면 발산, 너무 작으면 학습이 느리거나 지역 최적점에 빠질 수 있습니다.
    *   **배치 크기 (Batch Size)**: 한 번의 가중치 업데이트에 사용되는 데이터 샘플의 수입니다.
    *   **에포크 수 (Number of Epochs)**: 전체 훈련 데이터셋을 몇 번 반복하여 학습할지 결정합니다.
    *   **옵티마이저 (Optimizer)**: 손실 함수를 최소화하기 위해 가중치를 업데이트하는 알고리즘입니다. (예: SGD, Adam, RMSprop)
    *   **활성화 함수 (Activation Function)**: 각 뉴런의 출력을 결정하는 함수입니다. (예: ReLU, Sigmoid, Tanh, LeakyReLU)
    *   **가중치 초기화 (Weight Initialization)**: 학습 시작 전 가중치를 어떤 값으로 설정할지 정하는 방법입니다.
    *   **규제 강도 (Regularization Strength)**: 과적합을 방지하기 위한 규제 기법(L1, L2 규제, 드롭아웃 등)의 강도입니다.
    *   **드롭아웃 비율 (Dropout Rate)**: 과적합을 막기 위해 학습 중 일부 뉴런을 무작위로 비활성화하는 비율입니다.

-   **네트워크 구조 관련 하이퍼파라미터**:
    *   **은닉층의 수 (Number of Hidden Layers)**.
    *   **각 은닉층의 뉴런(유닛) 수 (Number of Units/Neurons per Layer)**.

-   **CNN (Convolutional Neural Network) 특화 하이퍼파라미터**:
    *   **필터(커널)의 개수 (Number of Filters/Kernels)**
    *   **필터(커널)의 크기 (Filter/Kernel Size)**
    *   **스트라이드 (Stride)**: 필터가 한 번에 이동하는 간격입니다.
    *   **패딩 (Padding)**: 컨볼루션 연산 시 입력 데이터 주변에 특정 값을 채워 크기를 조절하는 방식입니다.
    *   **풀링(Pooling) 방식 및 크기**: (예: Max Pooling, Average Pooling, 풀링 윈도우 크기)

-   **RNN (Recurrent Neural Network) 고유 하이퍼파라미터**:
    *   **RNN 유닛/셀 종류 (Type of RNN Unit/Cell)**: 순환 계산을 수행하는 내부 구조를 결정합니다.
        *   예: 바닐라 RNN, LSTM, GRU.
    *   **은닉 상태의 차원 수 (Dimensionality of Hidden State / Number of Hidden Units)**: 각 시점(time step)에서 RNN 유닛이 가지는 '메모리'의 크기 또는 정보 표현 능력입니다.
    *   **양방향성 (Bidirectionality) 여부**: 시퀀스 정보를 순방향뿐 아니라 역방향으로도 함께 처리할지 결정합니다. (예: 양방향 LSTM 사용)

-   **트랜스포머 특화**:
    *   **모델 차원 (d_model)**: 임베딩 및 어텐션 내부 차원.
    *   **어텐션 헤드 수 (Number of Attention Heads)**: Multi-Head Attention의 병렬 어텐션 개수.
    *   인코더/디코더 레이어 수.
    *   피드 포워드 네트워크 내부 차원 (d_ff).
    *   웜업 스텝 수 (학습률 스케줄링).

이러한 하이퍼파라미터들은 모델의 성능에 큰 영향을 미치므로, 적절한 값을 찾는 과정(하이퍼파라미터 튜닝)이 매우 중요합니다.
