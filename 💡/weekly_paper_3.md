---
title: "weekly_paper_3"
description: "weekly_paper_3.md | (2025 May 26 Monday)"
date: 2025-05-26T04:00:42+09:00
lastmod: 2025-05-27T18:35:55+09:00
categories: 
- 
tags: 
- 
series:
- weekly paper
draft: false
private: false
---

# Weekly Paper 3

## TOC
- [Weekly Paper 3](#weekly-paper-3)
  - [🇶 결정 트리의 장점과 단점은 무엇인가요?](#)
  - [🇶 부스팅은 어떤 특징을 가진 앙상블 기법인가요?](#)
  - [🇶 차원 축소 기법인 주성분 분석과 요인 분석의 차이는 무엇인지 설명해 주세요.](#)


## 🇶 결정 트리의 장점과 단점은 무엇인가요?

-   **장점**:
    *   **이해와 해석이 쉬움**: 모델의 의사결정 과정을 시각적으로 표현(트리 구조)할 수 있어 직관적입니다.  
        (마치 스무고개 게임 규칙을 보는 것과 비슷)
    *   **전처리 용이성**: 수치형/범주형 변수 모두 사용 가능하며, 데이터 스케일링이나 정규화에 크게 영향을 받지 않습니다.
    *   **비선형 관계 학습 가능**: 복잡한 데이터 패턴도 학습할 수 있습니다.

-   **단점**:
    *   **과적합(Overfitting) 경향**: 훈련 데이터에 과도하게 최적화되어 새로운 데이터에 대한 일반화 성능이 떨어질 수 있습니다. (특히 트리가 깊어질수록)


## 🇶 부스팅은 어떤 특징을 가진 앙상블 기법인가요?

-   **부스팅(Boosting)이란?**:
    *   여러 개의 약한 학습기(weak learner, 보통 성능이 약간 좋은 모델)를 **순차적으로 학습**시켜 결합하는 앙상블 기법입니다.
    *   **핵심 아이디어**: 이전 모델이 잘못 예측한 부분(오차)에 **가중치를 부여**하여 다음 모델이 이 오차를 더 잘 학습하도록 유도합니다. (마치 오답노트를 다음 학생에게 넘겨주며 "이 부분은 특히 틀렸으니 더 신경 써서 풀어봐"라고 하는 것과 비슷)
    *   강화학습과는 다른 지도 학습의 한 종류입니다.

-   **AdaBoost 외 부스팅 모델 예시**:
    1.  **GBM (Gradient Boosting Machine)**:
        *   **특징**: AdaBoost처럼 가중치를 업데이트하는 대신, 이전 모델의 잔여 오차(residual error)를 새로운 타겟으로 삼아 다음 모델이 이를 예측하도록 학습합니다. 경사 하강법(Gradient Descent)을 사용하여 손실 함수를 최소화합니다.
        *   **장점**: 유연성이 높고 다양한 손실 함수를 사용할 수 있습니다.
        *   **단점**: AdaBoost보다 파라미터 튜닝이 더 필요할 수 있고, 과적합에 민감할 수 있습니다.

    2.  **XGBoost (Extreme Gradient Boosting)**:
        *   **특징**:
            *   GBM을 기반으로 하며, 병렬 처리, 규제(Regularization), 가지치기(Pruning) 등을 통해 성능과 속도를 크게 향상했습니다.
            *   결측값을 자체적으로 처리할 수 있습니다.
            *   트리 생성 시 손실 함수에 규제 항을 추가하여 과적합을 방지합니다.
        *   **장점**: 매우 높은 예측 성능, 빠른 학습 속도, 과적합 방지 기능.
        *   **단점**: GBM보다 파라미터가 많고 복잡하여 튜닝이 어려울 수 있습니다.

    3.  **LightGBM (Light Gradient Boosting Machine)**:
        *   **특징**:
            *   XGBoost와 유사하지만, 리프 중심 트리 분할(leaf-wise tree growth) 방식을 사용하여 더 빠르고 효율적입니다. (XGBoost는 주로 레벨 중심 분할)
            *   대용량 데이터 처리에 유리합니다.
        *   **장점**: XGBoost보다 학습 속도가 빠르고 메모리 사용량이 적습니다.
        *   **단점**: 작은 데이터셋에서는 과적합되기 쉬울 수 있습니다.

    4.  **CatBoost (Categorical Boosting)**:
        *   **특징**:
            *   범주형 변수 처리에 특화되어 있습니다. (원-핫 인코딩 같은 전처리 없이 효과적으로 처리)
            *   대칭적인 트리(oblivious trees)를 사용하여 과적합을 줄이고 예측 속도를 높입니다.
        *   **장점**: 범주형 변수가 많은 데이터에서 우수한 성능, 사용 편의성.
        *   **단점**: 다른 부스팅 모델에 비해 학습 시간이 다소 걸릴 수 있습니다.

## 🇶 차원 축소 기법인 주성분 분석과 요인 분석의 차이는 무엇인지 설명해 주세요.

-   **차원 축소의 목적**: 데이터의 중요한 정보는 최대한 유지하면서 변수의 개수(차원)를 줄이는 것.

-   **주성분 분석 (PCA, Principal Component Analysis)**:
    *   **목표**: 데이터의 **분산을 최대한 보존**하는 새로운 축(주성분)을 찾는 것. 이 주성분들은 원래 변수들의 선형 결합으로 표현됩니다.
    *   **핵심 아이디어**: 데이터가 가장 넓게 퍼져 있는 방향(분산이 가장 큰 방향)을 첫 번째 주성분으로 삼고, 그 다음 직교하는 방향 중 분산이 큰 방향을 다음 주성분으로 선택합니다.
    *   **가정**: 주성분들이 관찰 변수들을 설명한다고 가정 (관찰 변수 = 주성분들의 함수).
    *   **비유**: 여러 각도에서 찍은 단체 사진들 중, 사람들의 모습이 가장 잘 드러나고 겹치지 않게 보이는 대표적인 각도(주성분) 몇 개를 선택하여 단체 사진의 특징을 요약하는 것과 비슷합니다.
    *   **주 사용**: 데이터 요약, 정보 압축, 노이즈 제거, 시각화.

-   **요인 분석 (FA, Factor Analysis)**:
    *   **목표**: 관찰된 변수들 간의 **상관관계 이면에 있는 잠재적인 구조나 공통 요인(latent factor)**을 찾아내는 것.
    *   **핵심 아이디어**: 관찰된 변수들은 소수의 잠재적인 요인들과 각 변수 고유의 오차에 의해 영향을 받는다고 가정합니다.
    *   **가정**: 잠재 요인들이 관찰 변수들을 야기(cause)한다고 가정 (잠재 요인 -> 관찰 변수).
    *   **비유**: 학생들의 국어, 영어, 수학 성적(관찰 변수)이 '언어 능력'과 '수리 능력'이라는 두 가지 잠재적인 학업 능력(요인)에 의해 결정된다고 보고, 이 잠재 능력을 찾아내는 것과 비슷합니다.
    *   **주 사용**: 변수들 간의 내재된 구조 파악, 심리학/사회과학 등에서 잠재 개념 측정, 설문조사 문항 타당성 검증.

-   **주요 차이점 요약**:
    *   **목적**: PCA는 분산 최대화(정보 요약), FA는 공분산 설명(잠재 구조 파악).
    *   **가정**: PCA는 주성분이 변수의 선형 결합, FA는 변수가 잠재 요인의 선형 결합 (인과관계 방향이 다름).
    *   **분산 처리**: PCA는 전체 분산을 고려, FA는 변수들 간의 공통 분산(공유되는 분산)에 집중.
*   
