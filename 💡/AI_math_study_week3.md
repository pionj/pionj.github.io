---
title: "AI_math_study_week3"
description: "AI_math_study_week3.md | (2025 May 26 Monday)"
date: 2025-05-26T03:59:59+09:00
lastmod: 2025-05-26T04:00:04+09:00
categories: 
- AI
tags: 
- 
series:
- ai math study
draft: false
private: false
---


# AI 수학 스터디 (3주차)

## TOC


# AI 수학 스터디 (3주차)

## TOC
- [AI 수학 스터디 (3주차)](#ai-수학-스터디-3주차)
  - [Prologue](#prologue)
  - [머신러닝 문제 해결의 구조](#h2-1)
  - [전통적이지만 매우 유용한 머신러닝 모델들](#h2-2)
  - [수치적 해 vs. 해석적 해](#h2-3)
  - [회귀: 수치 값 예측하기](#h2-4)
    - [훈련 함수 (Training Function)](#h2-4-1)
      - [파라미터 모델 vs. 비파라미터 모델](#h2-4-1-1)
    - [손실 함수 (Loss Function)](#h2-4-2)
      - [예측값 vs. 실제값](#h2-4-2-1)
      - [절댓값 거리 vs. 제곱 거리](#h2-4-2-2)
      - [특이점을 가진 함수들](#h2-4-2-3)
      - [선형 회귀의 손실 함수: 평균 제곱 오차 (MSE)](#h2-4-2-4)
      - [훈련, 검증, 테스트 데이터셋](#h2-4-2-5)
    - [최적화 (Optimization)](#h2-4-3)
      - [볼록(Convex) 함수 vs. 비볼록(Non-convex) 함수](#h2-4-3-1)
      - [함수의 최소/최대점을 찾는 방법](#h2-4-3-2)
      - [미적분 간략 정리](#h2-4-3-3)
      - [평균 제곱 오차 손실 함수 최소화하기](#h2-4-3-4)
  - [Summary](#summary)
  - [Epilogue](#epilogue)

## Prologue
지난 2장에서는 AI의 심장인 '데이터'와 그 데이터를 이해하기 위한 '확률 및 통계'라는 언어를 배웠다. 이번 3장, "Fitting Functions to Data"에서는 드디어 이 데이터에 '생명'을 불어넣는 작업, 즉 **데이터에 적절한 수학 함수를 맞추는(fitting)** 과정에 대해 본격적으로 탐구한다. 이는 수많은 AI 애플리케이션, 특히 신경망의 수학적 엔진의 핵심 아이디어를 이해하는 데 매우 중요한 단계다. 저자는 이 과정을 통해 "오늘 잘 맞는 함수가 내일도 잘 맞을까?"라는 근본적인 질문을 던지며 시작한다.

## 머신러닝 문제 해결의 구조 [🔝](#toc) <a id="h2-1"></a>
저자는 머신러닝을 이용한 AI 문제 해결 과정을 다음과 같은 구조로 설명한다. 이 구조는 앞으로 배울 신경망을 포함한 다양한 모델에 공통적으로 적용된다:

1.  **문제 정의 (Identify the problem):** 이미지 분류, 문서 분류, 주택 가격 예측, 사기 탐지 등 구체적인 사용 사례에 따라 해결하고자 하는 문제를 명확히 한다.
2.  **적절한 데이터 확보 (Acquire the appropriate data):** 모델이 '올바른 일'을 하도록 학습시키기 위한 데이터를 준비한다. 데이터는 깨끗하고, 완전하며, 필요에 따라 정규화, 표준화 등의 변환 과정을 거쳐야 한다. 이 단계는 모델 구현 및 훈련보다 훨씬 더 많은 시간이 소요될 수 있다.
3.  **가설 함수 생성 (Create a hypothesis function):** **훈련 함수(training function), 학습 함수(learning function), 예측 함수(prediction function), 모델(model)** 등으로 다양하게 불리는 이 함수는 관찰된 데이터를 설명하고, 새로운 데이터에 대한 예측을 수행하는 수학적 표현이다.
4.  **가중치(weights)의 수치 값 찾기 (Find the numerical values of weights):** 많은 모델(특히 신경망)은 훈련 함수 내에 미지의 파라미터, 즉 '가중치'를 가지고 있다. 데이터로부터 이 가중치들의 최적값을 찾는 것이 목표다.
5.  **오차 함수 생성 (Create an error function):** **오차 함수(error function), 비용 함수(cost function), 목적 함수(objective function), 손실 함수(loss function)** 등으로 불리는 이 함수는 모델의 예측과 실제값(ground truth) 사이의 거리를 측정한다. 목표는 이 손실 함수를 최소화하는 가중치 값을 찾는 것이다. 즉, 수학적 **최적화(optimization)** 문제를 푸는 것이다.
6.  **수학적 공식 결정 (Decide on mathematical formulas):** 훈련 함수, 손실 함수, 최적화 방법 등에 대한 수학적 공식을 엔지니어가 결정한다. 다양한 선택지가 있으며, 최종 판단은 배포된 모델의 성능에 달려있다.
7.  **최소화 지점 탐색 방법 찾기 (Find a way to search for minimizers):** 손실 함수를 최소화하는 특별한 가중치 값을 효율적으로 찾는 방법이 필요하다. **경사 하강법(gradient descent)**이 핵심적인 역할을 한다.
8.  **역전파 알고리즘 사용 (Use the backpropagation algorithm):** 데이터셋이 방대하고 모델이 다층 신경망인 경우, 경사 하강법에 필요한 미분값을 효율적으로 계산하기 위해 역전파 알고리즘을 사용한다. (4장에서 자세히 다룸)
9.  **함수 정규화 (Regularize a function):** 모델이 주어진 학습 데이터에 너무 잘 맞춰져서(과적합, overfitting), 새로운 데이터에서는 성능이 떨어지는 것을 방지하기 위해 함수를 '부드럽게' 만드는 정규화 기법을 사용한다. (4장에서 자세히 다룸)

## 전통적이지만 매우 유용한 머신러닝 모델들 [🔝](#toc) <a id="h2-2"></a>
이 장에서 다루는 데이터는 모두 정답(label)이 있는 **지도 학습(supervised learning)**에 해당하며, 모델의 목표는 새롭고 보지 못한 데이터의 레이블을 예측하는 것이다. 저자는 최신 AI 기술도 중요하지만, 일반적인 비즈니스 환경에서는 다음과 같은 전통적인 머신러닝 모델부터 시작하는 것이 좋다고 조언한다:

*   **선형 회귀 (Linear regression):** 수치 값을 예측한다.
*   **로지스틱 회귀 (Logistic regression):** 두 개의 클래스로 분류한다 (이진 분류).
*   **소프트맥스 회귀 (Softmax regression):** 여러 개의 클래스로 분류한다.
*   **서포트 벡터 머신 (Support vector machines, SVM):** 두 클래스 분류 또는 회귀.
*   **결정 트리 (Decision trees):** 다중 클래스 분류 또는 회귀.
*   **랜덤 포레스트 (Random forests):** 다중 클래스 분류 또는 회귀 (결정 트리의 앙상블).
*   **모델 앙상블 (Ensembles of models):** 여러 모델의 예측 결과를 평균내거나 투표하여 최종 결정을 내리는 방식.
*   **k-평균 군집화 (k-means clustering):** 여러 클래스로 분류 또는 회귀 (비지도 학습 방식이지만 분류/회귀 문제에도 적용 가능).

저자는 실제 현업에서 데이터 과학자 시간의 약 5%만이 모델 훈련에 사용되고, 대부분은 데이터 확보, 정제, 파이프라인 구축 등에 소요된다는 점을 강조하며, 머신러닝 모델 학습은 전체 파이프라인의 한 단계일 뿐임을 상기시킨다.

## 수치적 해 vs. 해석적 해 [🔝](#toc) <a id="h2-3"></a>
수학 문제를 푸는 방법에는 크게 두 가지 접근 방식이 있다:

*   **수치적 해 (Numerical solutions):** 근사적인 해를 숫자로 계산하는 방식이다. 이산화(discretization) 과정을 거쳐 컴퓨터로 시뮬레이션하며, 해석적 해가 없거나 구하기 어려운 복잡한 문제에 유용하다. 다만, 근사해이므로 오차가 존재할 수 있다.
*   **해석적 해 (Analytical solutions):** 수학적 분석을 통해 문제의 정확한 해를 공식이나 명시적인 형태로 찾는 방식이다. 강력하고 엄밀하지만, 고도의 수학적 지식과 전문성이 필요하며, 모든 문제에 대해 해석적 해를 구할 수 있는 것은 아니다.

대부분의 머신러닝 모델은 손실 함수의 최소점을 찾기 위해 수치적 해법, 특히 경사 하강법을 사용한다. 하지만 이 장에서 가장 먼저 다룰 **선형 회귀**의 경우, 단순성 덕분에 해석적 해를 통해 가중치를 직접 계산할 수 있다.

## 회귀: 수치 값 예측하기 [🔝](#toc) <a id="h2-4"></a>
회귀 분석은 AI 모델과 애플리케이션의 기초가 되는 매우 중요한 개념이다. 저자는 캐글의 '물고기 시장(Fish Market)' 데이터셋을 예시로 사용한다. 목표는 물고기의 5가지 길이 특징(Length1, Length2, Length3, Height, Width)을 사용하여 물고기의 무게(Weight)를 예측하는 모델을 만드는 것이다. (간단하게 하기 위해 물고기 종류(Species)라는 범주형 특징은 제외한다.)
결국, `무게 = f(길이1, 길이2, 길이3, 높이, 너비)` 형태의 함수 `f`를 찾는 것이 목표다.

### 훈련 함수 (Training Function) [🔝](#toc) <a id="h2-4-1"></a>
데이터 탐색 결과, 물고기 무게는 길이 특징들과 **선형적 관계**를 가진다고 가정한다 (실제로는 비선형 모델이 더 좋을 수 있지만, 여기서는 설명을 위해 선형으로 가정). 따라서 훈련 함수는 다음과 같이 정의된다:
`y = w0 + w1*x1 + w2*x2 + w3*x3 + w4*x4 + w5*x5`
여기서 `y`는 예측 무게, `x1`~`x5`는 5가지 길이 특징들, `w0`는 편향(bias) 항, `w1`~`w5`는 각 특징에 대한 가중치(weights)다.
우리의 목표는 주어진 데이터를 가장 잘 설명하는 최적의 `w` 값들을 찾는 것이다. 이 과정을 **모델 훈련(training the model)**이라고 한다.

#### 파라미터 모델 vs. 비파라미터 모델 [🔝](#toc) <a id="h2-4-1-1"></a>
*   **파라미터 모델 (Parametric models):** 위 선형 회귀 모델처럼, 훈련 함수의 형태(공식)가 미리 정해져 있고, 훈련 과정은 이 공식 내의 파라미터(가중치 `w`) 값을 찾는 데 집중된다.
*   **비파라미터 모델 (Nonparametric models):** 결정 트리나 랜덤 포레스트처럼, 훈련 함수의 형태가 미리 고정되어 있지 않고 데이터에 따라 모델 구조와 파라미터 수가 결정된다. 데이터에 과도하게 적응하여 과적합될 위험이 있으므로 주의해야 한다.

두 종류의 모델 모두 훈련 과정에서 조정해야 하는 추가적인 **하이퍼파라미터(hyperparameters)**를 가질 수 있다. 이는 훈련 함수 자체의 공식에 포함되지 않는 파라미터들이다.

### 손실 함수 (Loss Function) [🔝](#toc) <a id="h2-4-2"></a>
훈련 함수에서 최적의 가중치 `w` 값을 찾기 위해서는, 모델의 예측이 실제값과 얼마나 차이가 나는지를 측정하는 **손실 함수(loss function)**를 정의해야 한다.

#### 예측값 vs. 실제값 [🔝](#toc) <a id="h2-4-2-1"></a>
예를 들어, `w` 값들을 임의로 설정한 후 첫 번째 물고기 데이터의 특징 값들을 훈련 함수에 넣어 예측 무게(`y_predict`)를 계산한다. 이 예측값과 실제 데이터에 기록된 무게(`y_true`) 사이의 차이가 바로 오차다.

#### 절댓값 거리 vs. 제곱 거리 [🔝](#toc) <a id="h2-4-2-2"></a>
예측값과 실제값 사이의 오차를 측정하는 방법에는 여러 가지가 있다. 가장 흔히 사용되는 두 가지는 다음과 같다:
*   **절댓값 거리 (Absolute value distance):** `|y_predict - y_true|`
*   **제곱 거리 (Squared distance):** `(y_predict - y_true)^2`

절댓값 함수 `|x|`는 `x=0`에서 뾰족한 점(미분 불가능)을 가지는 반면, 제곱 함수 `x^2`는 모든 점에서 부드럽게 미분 가능하다. 이 차이는 최적화 알고리즘(특히 경사 하강법)을 사용할 때 중요하다. 또한, 제곱 거리는 큰 오차에 대해 더 큰 패널티를 부여하므로, 이상치(outlier)에 더 민감하게 반응한다.

#### 특이점을 가진 함수들 [🔝](#toc) <a id="h2-4-2-3"></a>
일반적으로 미분 가능한 함수는 그래프에 뾰족한 점(cusp, kink, corner)이 없다. 만약 함수가 이런 특이점(singularity)을 가지면 해당 지점에서 미분값을 정의할 수 없다. 이는 미분에 기반한 최적화 방법에 문제를 일으킬 수 있다 (어떤 미분값을 사용해야 할지, 불안정성 등). 그럼에도 불구하고, 머신러닝에서는 특이점을 가진 함수들이 종종 사용된다 (예: ReLU 활성화 함수, 절댓값 손실 함수, Lasso 회귀).

#### 선형 회귀의 손실 함수: 평균 제곱 오차 (MSE) [🔝](#toc) <a id="h2-4-2-4"></a>
선형 회귀에서는 일반적으로 **평균 제곱 오차(Mean Squared Error, MSE)**를 손실 함수로 사용한다. 이는 `m`개의 데이터 포인트에 대해 예측 오차의 제곱합을 평균낸 것이다:
`MSE = (1/m) * Σ (y_predict_i - y_true_i)^2` (i는 1부터 m까지)
벡터와 행렬을 사용하는 선형 대수 표기법으로는 더 간결하게 표현할 수 있다:
`MSE = (1/m) * ||y_predict_vec - y_true_vec||^2`
여기서 `||v||^2`는 벡터 `v`의 각 성분을 제곱하여 더한 값 (l2-norm의 제곱)을 의미한다.

#### 훈련, 검증, 테스트 데이터셋 [🔝](#toc) <a id="h2-4-2-5"></a>
손실 함수를 계산할 때 어떤 데이터 포인트를 사용할 것인가? 전체 데이터셋을 사용하는가, 아니면 일부만 사용하는가? 일반적으로 데이터셋을 세 부분으로 나눈다:
*   **훈련 데이터셋 (Training set):** 모델의 가중치를 학습하는 데 사용된다. 손실 함수는 주로 이 훈련 데이터셋에 대해 계산된다.
*   **검증 데이터셋 (Validation set):** 훈련된 여러 모델(또는 하이퍼파라미터 조합) 중에서 최적의 모델을 선택하는 데 사용된다. 훈련 과정에서 주기적으로 검증 데이터셋에 대한 성능을 평가하여 과적합 여부를 판단하고 모델을 조정한다.
*   **테스트 데이터셋 (Test set):** 최종적으로 선택된 모델의 일반화 성능을 평가하는 데 사용된다. 이 데이터셋은 모델 훈련이나 선택 과정에 전혀 사용되지 않아야 한다.

### 최적화 (Optimization) [🔝](#toc) <a id="h2-4-3"></a>
손실 함수를 정의했다면, 다음 단계는 이 손실 함수를 **최소화**하는 가중치 `w` 값을 찾는 것이다. 이것이 바로 **최적화(optimization)** 문제다.

#### 볼록(Convex) 함수 vs. 비볼록(Non-convex) 함수 [🔝](#toc) <a id="h2-4-3-1"></a>
*   **볼록 함수:** 그릇처럼 아래로 오목한 형태의 함수. 단 하나의 국소 최소점(local minimum)을 가지며, 이 점이 곧 전역 최소점(global minimum)이 된다. 최적화하기 상대적으로 쉽다. 선형 회귀의 MSE 손실 함수는 (특정 조건 하에) 볼록 함수다.
*   **비볼록 함수:** 여러 개의 국소 최소점을 가질 수 있으며, 울퉁불퉁한 지형과 같다. 경사 하강법과 같은 알고리즘은 국소 최소점에 빠져 전역 최소점을 찾지 못할 수 있다. 신경망의 손실 함수는 대부분 비볼록 함수다.

#### 함수의 최소/최대점을 찾는 방법 [🔝](#toc) <a id="h2-4-3-2"></a>
함수의 최소점이나 최대점(극점, extremum)에서는 함수의 기울기, 즉 **도함수(derivative)** 값이 0이 된다 (함수가 수평이 되는 지점). 따라서 손실 함수의 도함수를 계산하여 0이 되는 지점을 찾으면 손실을 최소화하는 가중치를 찾을 수 있다.

#### 미적분 간략 정리 [🔝](#toc) <a id="h2-4-3-3"></a>
저자는 미적분의 핵심 아이디어를 '변화율 측정'이라는 관점에서 간략히 복습한다.
*   **도함수(Derivative):** 함수가 특정 지점에서 얼마나 빠르게 변하는지를 나타내는 값. 그래프 상에서는 접선의 기울기에 해당한다.
*   **편도함수(Partial Derivative):** 여러 변수를 가진 함수에서, 특정 변수에 대해서만 미분하고 나머지 변수는 상수로 취급하는 것.
*   **기울기(Gradient):** 다변수 함수의 각 변수에 대한 편도함수들을 모아놓은 벡터. 함수의 가장 가파른 증가 방향을 나타낸다. 손실 함수를 최소화하려면 기울기의 반대 방향으로 이동해야 한다.

#### 평균 제곱 오차 손실 함수 최소화하기 [🔝](#toc) <a id="h2-4-3-4"></a>
선형 회귀의 MSE 손실 함수는 가중치 `w`에 대해 미분하여 0으로 설정함으로써 최소점을 해석적으로 찾을 수 있다. (이 과정은 행렬 미분을 포함하며, 책에서는 결과를 제시한다.)
이를 통해 얻어지는 가중치 `w`의 공식은 **정규 방정식(Normal Equation)**이라고 불린다. 이 공식을 사용하면 경사 하강법과 같은 반복적인 최적화 과정 없이 한 번에 최적의 가중치를 계산할 수 있다. 하지만 이 방법은 특징의 수가 매우 많거나 데이터가 매우 클 때 계산 비용이 높아질 수 있다는 단점이 있다. 또한, 특정 행렬의 역행렬을 계산해야 하는데, 이 역행렬이 존재하지 않는 경우(예: 특징들 간에 다중공선성이 심할 때)에는 사용할 수 없다. 이런 경우에는 정규화(regularization) 기법을 추가하거나 경사 하강법을 사용해야 한다.


## 로지스틱 회귀: 두 개의 클래스로 분류하기 [🔝](#toc) <a id="h2-5"></a>
로지스틱 회귀는 주로 **이진 분류(binary classification)** 작업에 사용된다 (예: 암/정상, 스팸/정상). 선형 회귀와 유사한 수학적 구조를 따르지만, 출력을 확률로 해석하기 위한 장치가 추가된다.

### 훈련 함수 [🔝](#toc) <a id="h2-5-1"></a>
선형 회귀처럼 특징들의 선형 결합에 편향을 더한 값(`s = w0 + w1*x1 + ... + wn*xn`)을 계산한 후, 이 결과를 직접 출력하는 대신 **로지스틱 함수(logistic function)** 또는 **시그모이드 함수(sigmoid function)**라 불리는 비선형 함수 `σ(s) = 1 / (1 + e^-s)`에 통과시킨다.
로지스틱 함수의 출력은 항상 0과 1 사이의 값을 가지므로, 특정 클래스에 속할 확률로 해석할 수 있다. 일반적으로 출력값이 0.5보다 크면 클래스 1, 작으면 클래스 0으로 분류한다 (0.5는 결정 임계값).
따라서 전체 훈련 함수는 `y_predict = Threshold(σ(w0 + w1*x1 + ... + wn*xn))` 형태가 된다.

### 손실 함수: 교차 엔트로피 (Cross-Entropy) [🔝](#toc) <a id="h2-5-2"></a>
분류 문제에서는 잘못 분류된 데이터 포인트에 패널티를 부과하는 손실 함수가 필요하다. 로지스틱 회귀에서는 **교차 엔트로피(cross-entropy) 손실 함수**가 널리 사용된다.
실제 레이블이 `y_true` (0 또는 1)이고, 로지스틱 함수의 출력이 `σ(s)`일 때, 하나의 데이터 포인트에 대한 비용(cost)은 다음과 같이 정의된다:
*   `y_true = 1`일 때: `-log(σ(s))` (σ(s)가 0에 가까우면 (잘못 예측) 비용이 커지고, 1에 가까우면 (잘 예측) 비용이 작아짐)
*   `y_true = 0`일 때: `-log(1 - σ(s))` (σ(s)가 1에 가까우면 (잘못 예측) 비용이 커지고, 0에 가까우면 (잘 예측) 비용이 작아짐)
이를 하나의 식으로 표현하면: `cost = -[y_true * log(σ(s)) + (1 - y_true) * log(1 - σ(s))]`
전체 훈련 데이터셋에 대한 교차 엔트로피 손실 함수는 각 데이터 포인트의 비용을 평균낸 것이다.

### 최적화 [🔝](#toc) <a id="h2-5-3"></a>
선형 회귀와 달리, 교차 엔트로피 손실 함수를 미분하여 0으로 설정해도 `w`에 대한 닫힌 형태의 해(closed-form solution)가 존재하지 않는다. 하지만 이 손실 함수는 **볼록(convex)** 함수이므로, 경사 하강법(또는 확률적/미니배치 경사 하강법)을 사용하면 (학습률이 적절하고 충분히 오래 기다린다면) 전역 최소점을 찾는 것이 보장된다. (4장에서 자세히 다룸)

## 소프트맥스 회귀: 다중 클래스로 분류하기 [🔝](#toc) <a id="h2-6"></a>
로지스틱 회귀를 여러 클래스로 확장한 것이 **소프트맥스 회귀(Softmax Regression)** 또는 **다항 로지스틱 회귀(Multinomial Logistic Regression)**다. MNIST 필기체 숫자 데이터셋(0~9까지 10개 클래스) 분류가 대표적인 예다.

### 훈련 함수 [🔝](#toc) <a id="h2-6-1"></a>
`k`개의 클래스가 있을 때, 각 클래스마다 별도의 선형 결합 점수 `s_j = w0_j + w1_j*x1 + ... + wn_j*xn` (j는 1부터 k까지)를 계산한다. 즉, 각 클래스마다 고유한 편향과 가중치 세트가 존재한다.
이 `k`개의 점수들을 **소프트맥스 함수(softmax function)**에 입력한다. 소프트맥스 함수는 로지스틱 함수를 다중 클래스로 일반화한 것으로, 각 클래스 `j`에 대한 확률 `σ(s)_j`를 다음과 같이 계산한다:
`σ(s)_j = e^(s_j) / (e^(s_1) + e^(s_2) + ... + e^(s_k))`
소프트맥스 함수의 출력은 각 클래스에 대한 확률 분포(모든 확률의 합은 1)를 나타낸다. 최종적으로 가장 높은 확률 값을 가진 클래스로 분류한다.
**주의:** 소프트맥스 회귀는 한 번에 하나의 클래스만 예측한다 (상호 배타적인 클래스). 이미지 안에 여러 객체를 동시에 분류하는 다중 출력(multi-output) 모델과는 다르다.

### 손실 함수: 일반화된 교차 엔트로피 [🔝](#toc) <a id="h2-6-2"></a>
로지스틱 회귀와 유사하게, 다중 클래스 분류에서도 **교차 엔트로피 손실 함수**를 사용한다. `y_true_j`는 실제 클래스가 `j`이면 1이고 아니면 0인 원-핫 인코딩된 레이블이다. 하나의 데이터 포인트에 대한 비용은 다음과 같다:
`cost = - Σ (y_true_j * log(σ(s)_j))` (j는 1부터 k까지)
이는 실제 클래스에 해당하는 소프트맥스 출력 확률의 로그값에만 음수를 취하는 것과 같다.

### 최적화 [🔝](#toc) <a id="h2-6-3"></a>
로지스틱 회귀와 마찬가지로, 소프트맥스 회귀의 교차 엔트로피 손실 함수도 `w`에 대한 닫힌 형태의 해가 없다. 하지만 이 함수 역시 볼록 함수이므로, 경사 하강법 계열의 알고리즘을 통해 최적의 가중치를 찾을 수 있다.

## 신경망의 마지막 계층으로 모델 통합하기 [🔝](#toc) <a id="h2-7"></a>
이 장에서 다룬 선형 회귀, 로지스틱 회귀, 소프트맥스 회귀 모델들은 데이터 특징들을 선형적으로 결합하는 단순한 구조를 가진다. 따라서 복잡한 비선형 관계를 포착하는 데는 한계가 있다.
신경망은 여러 계층에 걸쳐 비선형 활성화 함수를 사용하여 이러한 한계를 극복한다. 흥미롭게도, 신경망의 **마지막 출력 계층(output layer)**은 우리가 방금 배운 모델들의 형태를 그대로 사용할 수 있다:
*   **회귀 문제 (수치 예측):** 마지막 계층은 선형 회귀 계층 (활성화 함수 없음).
*   **이진 분류 문제:** 마지막 계층은 로지스틱 회귀 계층 (로지스틱 활성화 함수).
*   **다중 클래스 분류 문제:** 마지막 계층은 소프트맥스 회귀 계층 (소프트맥스 활성화 함수).
즉, 신경망의 이전 계층들이 데이터로부터 고차원의 복잡한 특징들을 추출하면, 마지막 계층은 이 특징들을 입력으로 받아 우리가 원하는 특정 작업을 수행하는 것이다. (5장에서 더 자세히 다룸)

## 기타 주요 머신러닝 기법 및 앙상블 [🔝](#toc) <a id="h2-8"></a>
회귀와 로지스틱/소프트맥스 회귀 외에도 강력하고 널리 사용되는 머신러닝 기법들이 있다.

### 서포트 벡터 머신 (Support Vector Machines, SVM) [🔝](#toc) <a id="h2-8-1"></a>
SVM은 분류 및 회귀 작업 모두에 사용될 수 있으며, 선형 또는 비선형 결정 경계를 찾는다. 분류 문제에서는 클래스 간의 **마진(margin)**을 최대로 하는 초평면(hyperplane)을 찾는 것을 목표로 한다.

#### 훈련 함수 [🔝](#toc) <a id="h2-8-1-1"></a>
선형 결합에 편향을 더한 값(`s = w^T x + w0`)을 계산한 후, 이 값의 부호(`sign(s)`)를 통해 클래스를 예측한다 (+1 또는 -1).

#### 손실 함수: 힌지 손실 (Hinge Loss) [🔝](#toc) <a id="h2-8-1-2"></a>
SVM은 **힌지 손실(hinge loss)**이라는 독특한 손실 함수를 사용한다:
`L = max(0, 1 - y_true * (w^T x + w0))`
이 손실 함수는 정확히 분류되고 마진 바깥쪽에 있는 데이터 포인트에는 패널티를 주지 않지만 (손실 0), 마진 안쪽에 있거나 잘못 분류된 포인트에는 선형적으로 증가하는 패널티를 부과한다.
여기에 마진의 너비를 제어하는 정규화 항 `λ||w||^2`이 추가된다. `λ`는 하이퍼파라미터다. 작은 `||w||`는 넓은 마진을 의미한다.

#### 최적화와 커널 트릭 [🔝](#toc) <a id="h2-8-1-3"></a>
SVM의 손실 함수는 볼록 함수이므로 국소 최소점에 대한 걱정은 없다. (첫 번째 항에 특이점이 있지만 서브그래디언트를 사용 가능)
SVM 최적화 문제(원 문제, primal problem)는 **쌍대 문제(dual problem)**로 변환하여 풀 수 있으며, 이는 특징의 수가 매우 많을 때 더 효율적일 수 있다. 이 과정에서 데이터 포인트들은 스칼라 곱(dot product) 형태로만 나타나는데, 이는 **커널 트릭(kernel trick)**을 사용할 수 있게 한다.
커널 트릭은 원래 데이터를 직접 고차원 공간으로 변환하지 않고도, 마치 고차원 공간에서 선형 분리를 수행하는 것과 같은 효과를 내는 기법이다. 즉, 저차원에서 비선형적으로 분리되는 데이터를 고차원으로 매핑하여 선형적으로 분리 가능하게 만든다. (예: 다항식 커널, 가우시안 커널)

### 결정 트리 (Decision Trees) [🔝](#toc) <a id="h2-8-2"></a>
결정 트리는 특징 값에 대한 일련의 질문(분기)을 통해 데이터를 분류하거나 예측값을 결정하는 나무 구조의 모델이다.

#### 훈련 함수 [🔝](#toc) <a id="h2-8-2-1"></a>
결정 트리는 입력으로 불리언(Boolean) 변수 (예: 특징 > 5, 특징 = sunny)를 받아 최종 결정을 내린다. 원래 특징이 불리언이 아니면 변환 과정이 필요하다. 함수 자체는 파라미터 `w`를 갖지 않는 **비파라미터 모델**이다. 데이터에 따라 나무의 구조가 결정된다.

#### 특징 선택, 엔트로피와 지니 불순도 [🔝](#toc) <a id="h2-8-2-2"></a>
결정 트리를 학습(성장)시킬 때, 각 노드에서 어떤 특징으로 분기할지를 결정해야 한다. 이때 **정보 이득(information gain)**을 최대화하거나 **불순도(impurity)**를 최소화하는 특징과 분기점을 선택한다.
*   **엔트로피(Entropy):** 시스템의 무질서도(불확실성)를 측정하는 정보 이론의 개념. 특정 특징으로 분기했을 때 엔트로피 감소량(정보 이득)이 가장 큰 특징을 선택한다.
*   **지니 불순도(Gini impurity):** 노드 내 데이터가 얼마나 잘 섞여 있는지를 측정. 지니 불순도를 가장 많이 낮추는 특징과 분기점을 선택한다. 지니 불순도가 계산 비용이 더 저렴하여 기본값으로 많이 사용된다.
결정 트리는 중요한 특징을 루트(root)에 가깝게 배치하므로 **특징 선택(feature selection)**에도 활용될 수 있다.

#### 회귀 결정 트리 [🔝](#toc) <a id="h2-8-2-3"></a>
결정 트리는 회귀 문제에도 사용될 수 있다. 이때는 정보 이득이나 지니 불순도 대신, 분기 후 자식 노드들의 **평균 제곱 오차(MSE)**를 최소화하는 특징과 분기점을 선택한다. 리프 노드(leaf node)의 예측값은 해당 노드에 속한 데이터 포인트들의 평균값이 된다.

#### 결정 트리의 단점 [🔝](#toc) <a id="h2-8-2-4"></a>
*   **불안정성:** 훈련 데이터에 작은 변화가 생겨도 트리 구조가 크게 바뀔 수 있다.
*   **회전 민감성:** 결정 경계가 주로 축에 평행하게 형성되므로 데이터 회전에 민감하다.
*   **과적합 경향:** 데이터에 과도하게 맞춰져 일반화 성능이 떨어질 수 있으므로 가지치기(pruning)가 필요하다.
*   **계산 비용 및 정확도:** 모든 특징과 가능한 분기점을 탐색하는 탐욕적(greedy) 알고리즘은 계산 비용이 높고 정확도가 떨어질 수 있다.

### 랜덤 포레스트 (Random Forests) [🔝](#toc) <a id="h2-8-3"></a>
랜덤 포레스트는 여러 개의 결정 트리를 **앙상블(ensemble)**한 모델이다. 각 트리는 훈련 데이터의 무작위 부분집합(부트스트랩 샘플링)과 특징의 무작위 부분집합을 사용하여 학습된다. 예측은 각 트리의 예측 결과를 평균(회귀)하거나 다수결 투표(분류)하여 결정한다. 이러한 무작위성은 개별 트리의 과적합을 줄이고 일반화 성능을 향상시키는 경향이 있다. 또한, 특징 중요도(feature importance)를 측정하는 데도 유용하다.

### k-평균 군집화 (k-means Clustering) [🔝](#toc) <a id="h2-8-4"></a>
k-평균 군집화는 대표적인 **비지도 학습(unsupervised learning)** 알고리즘으로, 데이터를 `k`개의 클러스터(군집)로 나눈다. 각 데이터 포인트는 가장 가까운 클러스터의 **중심(centroid)**에 할당된다. 클러스터 내 분산(중심까지의 제곱 유클리드 거리 합)을 최소화하는 것을 목표로 하며, 다음과 같은 반복적인 과정으로 수행된다:
1.  초기 `k`개의 중심을 설정한다. (초기화 방법 중요)
2.  각 데이터 포인트를 가장 가까운 중심에 할당한다.
3.  각 클러스터의 새로운 중심을 계산한다.
데이터 포인트의 클러스터 할당이 더 이상 변하지 않을 때까지 2, 3단계를 반복한다.

## 분류 모델의 성능 지표 [🔝](#toc) <a id="h2-9"></a>
회귀 모델의 성능은 예측값과 실제값 사이의 거리(예: MSE)로 비교적 쉽게 측정할 수 있지만, 분류 모델의 성능 평가는 더 다양한 측면을 고려해야 한다. 특히, 어떤 유형의 오류(예: 정상인데 암으로 진단 vs. 암인데 정상으로 진단)가 더 치명적인지에 따라 적절한 지표를 선택해야 한다.
주요 분류 성능 지표는 다음과 같다:
*   **정확도 (Accuracy):** 전체 예측 중 올바르게 예측한 비율. `(TP + TN) / (TP + TN + FP + FN)` (TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative)
*   **혼동 행렬 (Confusion Matrix):** 실제 클래스와 예측 클래스를 교차하여 보여주는 행렬. TP, TN, FP, FN 값을 직관적으로 파악할 수 있다.
*   **정밀도 (Precision):** 양성으로 예측한 것 중 실제 양성인 비율. `TP / (TP + FP)`
*   **재현율 (Recall) / 민감도 (Specificity):**
    *   **재현율(Recall):** 실제 양성인 것 중 양성으로 예측한 비율. `TP / (TP + FN)`
    *   **특이도(Specificity):** 실제 음성인 것 중 음성으로 예측한 비율. `TN / (TN + FP)`
*   **F1 점수 (F1 Score):** 정밀도와 재현율의 조화 평균. `2 * (Precision * Recall) / (Precision + Recall)`
*   **AUC (Area Under the Curve) 와 ROC (Receiver Operating Characteristics) 곡선:** 분류 모델의 결정 임계값을 다양하게 변화시키면서 TPR(재현율)과 FPR(1-특이도)의 관계를 그린 곡선. AUC는 이 곡선 아래 면적으로, 1에 가까울수록 모델 성능이 좋음을 의미한다.


## Summary
3장에서는 AI 모델링의 핵심 과정인 "데이터에 함수 맞추기"의 전체적인 구조를 살펴보았다. 문제 정의부터 데이터 확보, 가설 함수(훈련 함수) 설정, 손실 함수 정의, 그리고 이 손실 함수를 최소화하기 위한 최적화 과정까지, 머신러닝 파이프라인의 주요 단계를 이해했다.
특히 회귀 문제, 그중에서도 가장 기본적인 선형 회귀를 통해 훈련 함수와 손실 함수(평균 제곱 오차)의 구체적인 형태를 배우고, 이를 최소화하는 가중치를 찾는 방법을 다루었다. 볼록 함수와 비볼록 함수의 개념, 미적분(도함수, 기울기)의 역할, 그리고 선형 회귀의 경우 해석적 해법인 정규 방정식이 존재함을 알게 되었다. 또한 파라미터 모델과 비파라미터 모델, 수치적 해와 해석적 해의 차이점 등 중요한 개념들을 접했다.
이 장에서 배운 훈련 함수-손실 함수-최적화의 흐름은 앞으로 배울 더 복잡한 모델, 특히 신경망을 이해하는 데 있어 핵심적인 뼈대가 될 것이다.

## Epilogue
2장에서 데이터와 확률/통계라는 '재료'를 준비했다면, 3장에서는 그 재료를 가지고 '요리(모델링)'를 시작하는 첫 단계를 밟았다. 선형 회귀라는 가장 기본적인 요리법을 통해 함수를 데이터에 '맞추는' 과정을 경험했다. 손실 함수라는 '맛 평가 기준'을 설정하고, 미분이라는 '요리 도구'를 사용하여 최적의 '레시피(가중치)'를 찾는 과정이 흥미로웠다.
물론 실제 AI 문제들은 훨씬 더 복잡한 재료와 요리법을 요구하겠지만, 오늘 배운 기본 원리는 변하지 않을 것이다. 다음 장에서는 오늘 배운 최적화 방법, 특히 경사 하강법과 역전파 알고리즘에 대해 더 깊이 파고들고, 모델이 데이터에 너무 과하게 맞춰지는 과적합 문제를 해결하기 위한 정규화 기법에 대해서도 배운다고 하니, 더욱 본격적인 AI 모델링의 세계로 들어갈 준비를 해야겠다.