---
title: "AI_math_study_week3"
description: "AI_math_study_week3.md | (2025 May 26 Monday)"
date: 2025-05-26T03:59:59+09:00
lastmod: 2025-05-26T04:00:04+09:00
categories: 
- AI
tags: 
- 
series:
- ai math study
draft: false
private: false
---


# AI 수학 스터디 (3주차)

## TOC


# AI 수학 스터디 (3주차)

## TOC
- [AI 수학 스터디 (3주차)](#ai-수학-스터디-3주차)
  - [Prologue](#prologue)
  - [머신러닝 문제 해결의 구조](#h2-1)
  - [전통적이지만 매우 유용한 머신러닝 모델들](#h2-2)
  - [수치적 해 vs. 해석적 해](#h2-3)
  - [회귀: 수치 값 예측하기](#h2-4)
    - [훈련 함수 (Training Function)](#h2-4-1)
      - [파라미터 모델 vs. 비파라미터 모델](#h2-4-1-1)
    - [손실 함수 (Loss Function)](#h2-4-2)
      - [예측값 vs. 실제값](#h2-4-2-1)
      - [절댓값 거리 vs. 제곱 거리](#h2-4-2-2)
      - [특이점을 가진 함수들](#h2-4-2-3)
      - [선형 회귀의 손실 함수: 평균 제곱 오차 (MSE)](#h2-4-2-4)
      - [훈련, 검증, 테스트 데이터셋](#h2-4-2-5)
    - [최적화 (Optimization)](#h2-4-3)
      - [볼록(Convex) 함수 vs. 비볼록(Non-convex) 함수](#h2-4-3-1)
      - [함수의 최소/최대점을 찾는 방법](#h2-4-3-2)
      - [미적분 간략 정리](#h2-4-3-3)
      - [평균 제곱 오차 손실 함수 최소화하기](#h2-4-3-4)
  - [Summary](#summary)
  - [Epilogue](#epilogue)

## Prologue
지난 2장에서는 AI의 심장인 '데이터'와 그 데이터를 이해하기 위한 '확률 및 통계'라는 언어를 배웠다. 이번 3장, "Fitting Functions to Data"에서는 드디어 이 데이터에 '생명'을 불어넣는 작업, 즉 **데이터에 적절한 수학 함수를 맞추는(fitting)** 과정에 대해 본격적으로 탐구한다. 이는 수많은 AI 애플리케이션, 특히 신경망의 수학적 엔진의 핵심 아이디어를 이해하는 데 매우 중요한 단계다. 저자는 이 과정을 통해 "오늘 잘 맞는 함수가 내일도 잘 맞을까?"라는 근본적인 질문을 던지며 시작한다.

## 머신러닝 문제 해결의 구조 [🔝](#toc) <a id="h2-1"></a>
저자는 머신러닝을 이용한 AI 문제 해결 과정을 다음과 같은 구조로 설명한다. 이 구조는 앞으로 배울 신경망을 포함한 다양한 모델에 공통적으로 적용된다:

1.  **문제 정의 (Identify the problem):** 이미지 분류, 문서 분류, 주택 가격 예측, 사기 탐지 등 구체적인 사용 사례에 따라 해결하고자 하는 문제를 명확히 한다.
2.  **적절한 데이터 확보 (Acquire the appropriate data):** 모델이 '올바른 일'을 하도록 학습시키기 위한 데이터를 준비한다. 데이터는 깨끗하고, 완전하며, 필요에 따라 정규화, 표준화 등의 변환 과정을 거쳐야 한다. 이 단계는 모델 구현 및 훈련보다 훨씬 더 많은 시간이 소요될 수 있다.
3.  **가설 함수 생성 (Create a hypothesis function):** **훈련 함수(training function), 학습 함수(learning function), 예측 함수(prediction function), 모델(model)** 등으로 다양하게 불리는 이 함수는 관찰된 데이터를 설명하고, 새로운 데이터에 대한 예측을 수행하는 수학적 표현이다.
4.  **가중치(weights)의 수치 값 찾기 (Find the numerical values of weights):** 많은 모델(특히 신경망)은 훈련 함수 내에 미지의 파라미터, 즉 '가중치'를 가지고 있다. 데이터로부터 이 가중치들의 최적값을 찾는 것이 목표다.
5.  **오차 함수 생성 (Create an error function):** **오차 함수(error function), 비용 함수(cost function), 목적 함수(objective function), 손실 함수(loss function)** 등으로 불리는 이 함수는 모델의 예측과 실제값(ground truth) 사이의 거리를 측정한다. 목표는 이 손실 함수를 최소화하는 가중치 값을 찾는 것이다. 즉, 수학적 **최적화(optimization)** 문제를 푸는 것이다.
6.  **수학적 공식 결정 (Decide on mathematical formulas):** 훈련 함수, 손실 함수, 최적화 방법 등에 대한 수학적 공식을 엔지니어가 결정한다. 다양한 선택지가 있으며, 최종 판단은 배포된 모델의 성능에 달려있다.
7.  **최소화 지점 탐색 방법 찾기 (Find a way to search for minimizers):** 손실 함수를 최소화하는 특별한 가중치 값을 효율적으로 찾는 방법이 필요하다. **경사 하강법(gradient descent)**이 핵심적인 역할을 한다.
8.  **역전파 알고리즘 사용 (Use the backpropagation algorithm):** 데이터셋이 방대하고 모델이 다층 신경망인 경우, 경사 하강법에 필요한 미분값을 효율적으로 계산하기 위해 역전파 알고리즘을 사용한다. (4장에서 자세히 다룸)
9.  **함수 정규화 (Regularize a function):** 모델이 주어진 학습 데이터에 너무 잘 맞춰져서(과적합, overfitting), 새로운 데이터에서는 성능이 떨어지는 것을 방지하기 위해 함수를 '부드럽게' 만드는 정규화 기법을 사용한다. (4장에서 자세히 다룸)

## 전통적이지만 매우 유용한 머신러닝 모델들 [🔝](#toc) <a id="h2-2"></a>
이 장에서 다루는 데이터는 모두 정답(label)이 있는 **지도 학습(supervised learning)**에 해당하며, 모델의 목표는 새롭고 보지 못한 데이터의 레이블을 예측하는 것이다. 저자는 최신 AI 기술도 중요하지만, 일반적인 비즈니스 환경에서는 다음과 같은 전통적인 머신러닝 모델부터 시작하는 것이 좋다고 조언한다:

*   **선형 회귀 (Linear regression):** 수치 값을 예측한다.
*   **로지스틱 회귀 (Logistic regression):** 두 개의 클래스로 분류한다 (이진 분류).
*   **소프트맥스 회귀 (Softmax regression):** 여러 개의 클래스로 분류한다.
*   **서포트 벡터 머신 (Support vector machines, SVM):** 두 클래스 분류 또는 회귀.
*   **결정 트리 (Decision trees):** 다중 클래스 분류 또는 회귀.
*   **랜덤 포레스트 (Random forests):** 다중 클래스 분류 또는 회귀 (결정 트리의 앙상블).
*   **모델 앙상블 (Ensembles of models):** 여러 모델의 예측 결과를 평균내거나 투표하여 최종 결정을 내리는 방식.
*   **k-평균 군집화 (k-means clustering):** 여러 클래스로 분류 또는 회귀 (비지도 학습 방식이지만 분류/회귀 문제에도 적용 가능).

저자는 실제 현업에서 데이터 과학자 시간의 약 5%만이 모델 훈련에 사용되고, 대부분은 데이터 확보, 정제, 파이프라인 구축 등에 소요된다는 점을 강조하며, 머신러닝 모델 학습은 전체 파이프라인의 한 단계일 뿐임을 상기시킨다.

## 수치적 해 vs. 해석적 해 [🔝](#toc) <a id="h2-3"></a>
수학 문제를 푸는 방법에는 크게 두 가지 접근 방식이 있다:

*   **수치적 해 (Numerical solutions):** 근사적인 해를 숫자로 계산하는 방식이다. 이산화(discretization) 과정을 거쳐 컴퓨터로 시뮬레이션하며, 해석적 해가 없거나 구하기 어려운 복잡한 문제에 유용하다. 다만, 근사해이므로 오차가 존재할 수 있다.
*   **해석적 해 (Analytical solutions):** 수학적 분석을 통해 문제의 정확한 해를 공식이나 명시적인 형태로 찾는 방식이다. 강력하고 엄밀하지만, 고도의 수학적 지식과 전문성이 필요하며, 모든 문제에 대해 해석적 해를 구할 수 있는 것은 아니다.

대부분의 머신러닝 모델은 손실 함수의 최소점을 찾기 위해 수치적 해법, 특히 경사 하강법을 사용한다. 하지만 이 장에서 가장 먼저 다룰 **선형 회귀**의 경우, 단순성 덕분에 해석적 해를 통해 가중치를 직접 계산할 수 있다.

## 회귀: 수치 값 예측하기 [🔝](#toc) <a id="h2-4"></a>
회귀 분석은 AI 모델과 애플리케이션의 기초가 되는 매우 중요한 개념이다. 저자는 캐글의 '물고기 시장(Fish Market)' 데이터셋을 예시로 사용한다. 목표는 물고기의 5가지 길이 특징(Length1, Length2, Length3, Height, Width)을 사용하여 물고기의 무게(Weight)를 예측하는 모델을 만드는 것이다. (간단하게 하기 위해 물고기 종류(Species)라는 범주형 특징은 제외한다.)
결국, `무게 = f(길이1, 길이2, 길이3, 높이, 너비)` 형태의 함수 `f`를 찾는 것이 목표다.

### 훈련 함수 (Training Function) [🔝](#toc) <a id="h2-4-1"></a>
데이터 탐색 결과, 물고기 무게는 길이 특징들과 **선형적 관계**를 가진다고 가정한다 (실제로는 비선형 모델이 더 좋을 수 있지만, 여기서는 설명을 위해 선형으로 가정). 따라서 훈련 함수는 다음과 같이 정의된다:
`y = w0 + w1*x1 + w2*x2 + w3*x3 + w4*x4 + w5*x5`
여기서 `y`는 예측 무게, `x1`~`x5`는 5가지 길이 특징들, `w0`는 편향(bias) 항, `w1`~`w5`는 각 특징에 대한 가중치(weights)다.
우리의 목표는 주어진 데이터를 가장 잘 설명하는 최적의 `w` 값들을 찾는 것이다. 이 과정을 **모델 훈련(training the model)**이라고 한다.

#### 파라미터 모델 vs. 비파라미터 모델 [🔝](#toc) <a id="h2-4-1-1"></a>
*   **파라미터 모델 (Parametric models):** 위 선형 회귀 모델처럼, 훈련 함수의 형태(공식)가 미리 정해져 있고, 훈련 과정은 이 공식 내의 파라미터(가중치 `w`) 값을 찾는 데 집중된다.
*   **비파라미터 모델 (Nonparametric models):** 결정 트리나 랜덤 포레스트처럼, 훈련 함수의 형태가 미리 고정되어 있지 않고 데이터에 따라 모델 구조와 파라미터 수가 결정된다. 데이터에 과도하게 적응하여 과적합될 위험이 있으므로 주의해야 한다.

두 종류의 모델 모두 훈련 과정에서 조정해야 하는 추가적인 **하이퍼파라미터(hyperparameters)**를 가질 수 있다. 이는 훈련 함수 자체의 공식에 포함되지 않는 파라미터들이다.

### 손실 함수 (Loss Function) [🔝](#toc) <a id="h2-4-2"></a>
훈련 함수에서 최적의 가중치 `w` 값을 찾기 위해서는, 모델의 예측이 실제값과 얼마나 차이가 나는지를 측정하는 **손실 함수(loss function)**를 정의해야 한다.

#### 예측값 vs. 실제값 [🔝](#toc) <a id="h2-4-2-1"></a>
예를 들어, `w` 값들을 임의로 설정한 후 첫 번째 물고기 데이터의 특징 값들을 훈련 함수에 넣어 예측 무게(`y_predict`)를 계산한다. 이 예측값과 실제 데이터에 기록된 무게(`y_true`) 사이의 차이가 바로 오차다.

#### 절댓값 거리 vs. 제곱 거리 [🔝](#toc) <a id="h2-4-2-2"></a>
예측값과 실제값 사이의 오차를 측정하는 방법에는 여러 가지가 있다. 가장 흔히 사용되는 두 가지는 다음과 같다:
*   **절댓값 거리 (Absolute value distance):** `|y_predict - y_true|`
*   **제곱 거리 (Squared distance):** `(y_predict - y_true)^2`

절댓값 함수 `|x|`는 `x=0`에서 뾰족한 점(미분 불가능)을 가지는 반면, 제곱 함수 `x^2`는 모든 점에서 부드럽게 미분 가능하다. 이 차이는 최적화 알고리즘(특히 경사 하강법)을 사용할 때 중요하다. 또한, 제곱 거리는 큰 오차에 대해 더 큰 패널티를 부여하므로, 이상치(outlier)에 더 민감하게 반응한다.

#### 특이점을 가진 함수들 [🔝](#toc) <a id="h2-4-2-3"></a>
일반적으로 미분 가능한 함수는 그래프에 뾰족한 점(cusp, kink, corner)이 없다. 만약 함수가 이런 특이점(singularity)을 가지면 해당 지점에서 미분값을 정의할 수 없다. 이는 미분에 기반한 최적화 방법에 문제를 일으킬 수 있다 (어떤 미분값을 사용해야 할지, 불안정성 등). 그럼에도 불구하고, 머신러닝에서는 특이점을 가진 함수들이 종종 사용된다 (예: ReLU 활성화 함수, 절댓값 손실 함수, Lasso 회귀).

#### 선형 회귀의 손실 함수: 평균 제곱 오차 (MSE) [🔝](#toc) <a id="h2-4-2-4"></a>
선형 회귀에서는 일반적으로 **평균 제곱 오차(Mean Squared Error, MSE)**를 손실 함수로 사용한다. 이는 `m`개의 데이터 포인트에 대해 예측 오차의 제곱합을 평균낸 것이다:
`MSE = (1/m) * Σ (y_predict_i - y_true_i)^2` (i는 1부터 m까지)
벡터와 행렬을 사용하는 선형 대수 표기법으로는 더 간결하게 표현할 수 있다:
`MSE = (1/m) * ||y_predict_vec - y_true_vec||^2`
여기서 `||v||^2`는 벡터 `v`의 각 성분을 제곱하여 더한 값 (l2-norm의 제곱)을 의미한다.

#### 훈련, 검증, 테스트 데이터셋 [🔝](#toc) <a id="h2-4-2-5"></a>
손실 함수를 계산할 때 어떤 데이터 포인트를 사용할 것인가? 전체 데이터셋을 사용하는가, 아니면 일부만 사용하는가? 일반적으로 데이터셋을 세 부분으로 나눈다:
*   **훈련 데이터셋 (Training set):** 모델의 가중치를 학습하는 데 사용된다. 손실 함수는 주로 이 훈련 데이터셋에 대해 계산된다.
*   **검증 데이터셋 (Validation set):** 훈련된 여러 모델(또는 하이퍼파라미터 조합) 중에서 최적의 모델을 선택하는 데 사용된다. 훈련 과정에서 주기적으로 검증 데이터셋에 대한 성능을 평가하여 과적합 여부를 판단하고 모델을 조정한다.
*   **테스트 데이터셋 (Test set):** 최종적으로 선택된 모델의 일반화 성능을 평가하는 데 사용된다. 이 데이터셋은 모델 훈련이나 선택 과정에 전혀 사용되지 않아야 한다.

### 최적화 (Optimization) [🔝](#toc) <a id="h2-4-3"></a>
손실 함수를 정의했다면, 다음 단계는 이 손실 함수를 **최소화**하는 가중치 `w` 값을 찾는 것이다. 이것이 바로 **최적화(optimization)** 문제다.

#### 볼록(Convex) 함수 vs. 비볼록(Non-convex) 함수 [🔝](#toc) <a id="h2-4-3-1"></a>
*   **볼록 함수:** 그릇처럼 아래로 오목한 형태의 함수. 단 하나의 국소 최소점(local minimum)을 가지며, 이 점이 곧 전역 최소점(global minimum)이 된다. 최적화하기 상대적으로 쉽다. 선형 회귀의 MSE 손실 함수는 (특정 조건 하에) 볼록 함수다.
*   **비볼록 함수:** 여러 개의 국소 최소점을 가질 수 있으며, 울퉁불퉁한 지형과 같다. 경사 하강법과 같은 알고리즘은 국소 최소점에 빠져 전역 최소점을 찾지 못할 수 있다. 신경망의 손실 함수는 대부분 비볼록 함수다.

#### 함수의 최소/최대점을 찾는 방법 [🔝](#toc) <a id="h2-4-3-2"></a>
함수의 최소점이나 최대점(극점, extremum)에서는 함수의 기울기, 즉 **도함수(derivative)** 값이 0이 된다 (함수가 수평이 되는 지점). 따라서 손실 함수의 도함수를 계산하여 0이 되는 지점을 찾으면 손실을 최소화하는 가중치를 찾을 수 있다.

#### 미적분 간략 정리 [🔝](#toc) <a id="h2-4-3-3"></a>
저자는 미적분의 핵심 아이디어를 '변화율 측정'이라는 관점에서 간략히 복습한다.
*   **도함수(Derivative):** 함수가 특정 지점에서 얼마나 빠르게 변하는지를 나타내는 값. 그래프 상에서는 접선의 기울기에 해당한다.
*   **편도함수(Partial Derivative):** 여러 변수를 가진 함수에서, 특정 변수에 대해서만 미분하고 나머지 변수는 상수로 취급하는 것.
*   **기울기(Gradient):** 다변수 함수의 각 변수에 대한 편도함수들을 모아놓은 벡터. 함수의 가장 가파른 증가 방향을 나타낸다. 손실 함수를 최소화하려면 기울기의 반대 방향으로 이동해야 한다.

#### 평균 제곱 오차 손실 함수 최소화하기 [🔝](#toc) <a id="h2-4-3-4"></a>
선형 회귀의 MSE 손실 함수는 가중치 `w`에 대해 미분하여 0으로 설정함으로써 최소점을 해석적으로 찾을 수 있다. (이 과정은 행렬 미분을 포함하며, 책에서는 결과를 제시한다.)
이를 통해 얻어지는 가중치 `w`의 공식은 **정규 방정식(Normal Equation)**이라고 불린다. 이 공식을 사용하면 경사 하강법과 같은 반복적인 최적화 과정 없이 한 번에 최적의 가중치를 계산할 수 있다. 하지만 이 방법은 특징의 수가 매우 많거나 데이터가 매우 클 때 계산 비용이 높아질 수 있다는 단점이 있다. 또한, 특정 행렬의 역행렬을 계산해야 하는데, 이 역행렬이 존재하지 않는 경우(예: 특징들 간에 다중공선성이 심할 때)에는 사용할 수 없다. 이런 경우에는 정규화(regularization) 기법을 추가하거나 경사 하강법을 사용해야 한다.

## Summary
3장에서는 AI 모델링의 핵심 과정인 "데이터에 함수 맞추기"의 전체적인 구조를 살펴보았다. 문제 정의부터 데이터 확보, 가설 함수(훈련 함수) 설정, 손실 함수 정의, 그리고 이 손실 함수를 최소화하기 위한 최적화 과정까지, 머신러닝 파이프라인의 주요 단계를 이해했다.
특히 회귀 문제, 그중에서도 가장 기본적인 선형 회귀를 통해 훈련 함수와 손실 함수(평균 제곱 오차)의 구체적인 형태를 배우고, 이를 최소화하는 가중치를 찾는 방법을 다루었다. 볼록 함수와 비볼록 함수의 개념, 미적분(도함수, 기울기)의 역할, 그리고 선형 회귀의 경우 해석적 해법인 정규 방정식이 존재함을 알게 되었다. 또한 파라미터 모델과 비파라미터 모델, 수치적 해와 해석적 해의 차이점 등 중요한 개념들을 접했다.
이 장에서 배운 훈련 함수-손실 함수-최적화의 흐름은 앞으로 배울 더 복잡한 모델, 특히 신경망을 이해하는 데 있어 핵심적인 뼈대가 될 것이다.

## Epilogue
2장에서 데이터와 확률/통계라는 '재료'를 준비했다면, 3장에서는 그 재료를 가지고 '요리(모델링)'를 시작하는 첫 단계를 밟았다. 선형 회귀라는 가장 기본적인 요리법을 통해 함수를 데이터에 '맞추는' 과정을 경험했다. 손실 함수라는 '맛 평가 기준'을 설정하고, 미분이라는 '요리 도구'를 사용하여 최적의 '레시피(가중치)'를 찾는 과정이 흥미로웠다.
물론 실제 AI 문제들은 훨씬 더 복잡한 재료와 요리법을 요구하겠지만, 오늘 배운 기본 원리는 변하지 않을 것이다. 다음 장에서는 오늘 배운 최적화 방법, 특히 경사 하강법과 역전파 알고리즘에 대해 더 깊이 파고들고, 모델이 데이터에 너무 과하게 맞춰지는 과적합 문제를 해결하기 위한 정규화 기법에 대해서도 배운다고 하니, 더욱 본격적인 AI 모델링의 세계로 들어갈 준비를 해야겠다.